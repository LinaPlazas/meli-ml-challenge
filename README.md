<h1 align="center">Meli ML Chanllenge üöÄ </h1> 

## Resumen: 
El presente proyecto tiene como fin plantear una posible soluci√≥n para el reto t√©cnico planteado, el cual consiste en dise√±ar y construir un sistema que realice las siguientes tareas a partir de diferentes documentos en formato pdf:<br> 

***1. Clasificaci√≥n autom√°tica por tipo de documento:*** <br>

   Cada archivo debe ser clasificado en una de las siguientes categorias: 
   - Contrato
   - Resoluci√≥n
   - Certificado
   - Factura
2. **Detecci√≥n de datos sensibles (PII)**
     
Identifica y resalta informaci√≥n confidencial como:
- Nombres completos  
- N√∫meros de identificaci√≥n  
- Correos electr√≥nicos  
- Tel√©fonos  
- Direcciones
  
3. **Identificaci√≥n de documentos duplicados o similares**
  
Detecta documentos duplicados por medio de un checksum MD5.

4. **Segmentaci√≥n y extracci√≥n de secciones de inter√©s (Opcional)**

Extrae de cada documento la secci√≥n espec√≠fica que contiene requisitos normativos,  
cuando exista, o marca el documento como ‚ÄúNo Aplica‚Äù.

## Arquitectura y dise√±o de la soluci√≥n

El siguiente diagrama resume c√≥mo est√° construida la presente soluci√≥n:

![Arquitectura](images/Arquitectura.png)

### Desarrollo y construcci√≥n de la aplicaci√≥n 

La aplicaci√≥n fue desarrollada en FastAPI y luego construida como una imagen de docker la cual se almacena en Amazon ECR para su posterior despliegue.

  
### Despliegue de la aplicaci√≥n 

En cuanto al despliegue se utiliz√≥ AWS ECS con Fargate como servicio de orquestaci√≥n para ejecutar la imagen en un entorno serverless, asimismo se utiliz√≥ un Load Balancer que permite el acceso externo a la API.

### Servicios externos conectados 

Se utiliza MongoDB como base de datos para almacenar el texto extra√≠do de los documentos, as√≠ como otros procesamientos realizados (clasificaci√≥n de documentos, extracci√≥n de PII, informaci√≥n de duplicados, extracci√≥n de secciones de inter√©s).

Por otro lado, tambi√©n se cuenta con un bucket de S3 para almacenar los documentos utilizados tanto en el entrenamiento del modelo, que se describe m√°s adelante, como en las diferentes pruebas de la aplicaci√≥n.

## Implementaci√≥n detallada de la soluci√≥n

Para el desarrollo de la aplicaci√≥n, se procur√≥ seguir principios de arquitectura limpia, organizando el proyecto en m√≥dulos con responsabilidades definidas. A continuaci√≥n se explicara cada uno de los modulos:

### Extracci√≥n de texto 
Para abordar el problema planteado, el primer paso realizado fue lograr la extracci√≥n de texto de archivos pdf, para esto se utiliz√≥ amazon textract debido a que ofrece una extracci√≥n robusta basada en machine learning,que no solo reconoce texto impreso, sino tambi√©n estructuras complejas como tablas y formularios.

De igual manera para enfrentar el problema de el procesamiento asincronico se utilizo start_document_text_detection, esta opci√≥n permite iniciar m√∫ltiples procesos de extracci√≥n en paralelo sin necesidad de esperar a que cada uno finalice antes de continuar con el siguiente.

Para gestionar este procesamiento concurrente, se implement√≥ una estrategia que permite lanzar varios trabajos de an√°lisis al mismo tiempo, hacer seguimiento de cada uno de ellos de forma no bloqueante, y finalmente recolectar los resultados una vez completados. Una vez obtenidos los textos de los archivos pdf, se almacenan en la base de datos, lo que permite su posterior uso en tareas como clasificaci√≥n autom√°tica, detecci√≥n de informaci√≥n sensible o an√°lisis de secciones normativas.

### Clasificaci√≥n en categorias

#### Pre procesamiento y entrenamiento del modelo

Con el fin de implementar el sistema clasificador de textos, fueron necesarios unos pasos previos al desarrollo del modulo. En primer lugar se recolectaron diferentes archivos pdf con el objetivo de armar un dataset, de esta manera los documentos referentes a contratos, resoluciones y certificados, se obtuvieron del repositorio de datos abiertos de Secop II [Datos abiertos Secop II](https://www.datos.gov.co/Gastos-Gubernamentales/SECOP-II-Contratos-Electr-nicos/jbjy-vk9h/about_data), de igual manera para las facturas, se utilizaron plantillas de ejemplo.

En total se obtuvieron la siguiente cantidad de documentos, la cantidad de paginas variaba por documento, algunos tenian hasta 15 p√°ginas:

Certificados: 49 <br>
Contratos:43 <br>
Resoluciones: 56 <br>
Facturas: 37 <br>

Una vez recolectados los documentos de entrenamiento, se procedi√≥ a construir el dataset en formato tabular, con el fin de utilizarlo como entrada para los modelos,Este proceso se llev√≥ a cabo mediante un notebook de Jupyter, ubicado en la ruta notebooks/create_tabular_dataset.py, la salida fue un archivo CSV que contiene los campos: filename, label (etiqueta correspondiente a la clase del documento) y text (texto extra√≠do del PDF utilizando Amazon Textract) y se encuentra en la ruta notebooks/data/dataset.csv

Una vez obtenido el dataset tabular, se procedi√≥ a realizar diferentes pruebas utilizando modelos de aprendizaje autom√°tico tradicional. Para ello, se implement√≥ una etapa de vectorizaci√≥n del texto puesto que  Los modelos no entienden palabras directamente, por lo cual la vectorizaci√≥n convierte el texto en representaciones num√©ricas que los algoritmos de machine learning pueden procesar. Se inici√≥ con un modelo de regresi√≥n log√≠stica debido a su simplicidad y buen desempe√±o en problemas de clasificaci√≥n de texto. Los datos se dividieron en un 80 % para entrenamiento y un 20 % para validaci√≥n. A continuaci√≥n, se presentan los resultados obtenidos:

<p align="center">
  <img src="images/mc_regresion1.png" alt="matrizrl1" width="300"/>
</p>

<p align="center">
  <img src="images/metricas_regresion1.png" alt="metricas1" width="300"/>
</p>


Aunque los resultados fueron prometedores, se decidi√≥ mejorar el preprocesamiento del texto incorporando una peque√±a etapa de limpieza y tokenizaci√≥n del texto utilizando la libreria de procesamiento de lenguaje natural nltk para optimizar la calidad de las caracter√≠sticas extra√≠das. Adem√°s se decidi√≥ evaluar varios modelos de clasificaci√≥n como adicional a la regresi√≥n logistica como Lineal SVM, Random Forest, Multinomial Naive Bayes, con el fin de comparar y elegir el que brindara mejores m√©tricas. A continuaci√≥n se muestran los resultados de cada uno de los modelos con la limpieza de texto realizada

Regresi√≥n Logistica

<p align="center">
  <img src="images/mc_regresion2.png" alt="matrizrl2" width="300"/>
</p>

<p align="center">
  <img src="images/metricas_regresion2.png" alt="metricas2" width="300"/>
</p>


SVM

<p align="center">
  <img src="images/mc_svm.png" alt="matrizsvm" width="300"/>
</p>

<p align="center">
  <img src="images/metricas_SVM.png" alt="metricas3" width="300"/>
</p>


Random Forest

<p align="center">
  <img src="images/mc_randomforest.png" alt="matrizrandom" width="300"/>
</p>

<p align="center">
  <img src="images/metricas_randomforest.png" alt="metricas4" width="300"/>
</p>

NB

<p align="center">
  <img src="images/mc_NB.png" alt="matriznb" width="300"/>
</p>

<p align="center">
  <img src="images/metricas_NB.png" alt="metricas1" width="300"/>
</p>

Como se puede observar, en t√©rminos generales y para todas las m√©tricas (Accuracy, Precision, Recall y F1-score), el modelo Random Forest alcanz√≥ un desempe√±o cercano al 97%. Por ello, fue seleccionado para su implementaci√≥n en el m√≥dulo de clasificaci√≥n de categor√≠as.

#### Implementaci√≥n modulo clasificador

Una vez obtenido el modelo clasificador, se desarroll√≥ el m√≥dulo encargado de determinar la categor√≠a que mejor se ajusta al documento. Para ello, se consulta en la base de datos el texto extra√≠do de los PDFs y se realiza la clasificaci√≥n correspondiente. Finalmente, el m√≥dulo almacena dos campos: la categor√≠a asignada y los scores, es decir, los puntajes de probabilidad asociados a cada una de las categor√≠as.

### Detecci√≥n de datos sensibles (PII)

Para este m√≥dulo se utiliz√≥ la biblioteca spaCy para el reconocimiento de entidades nombradas (NER), espec√≠ficamente para la categor√≠a de nombres propios. De igual manera, se implement√≥ el modelo es_core_news_lg de la misma biblioteca, el cual proporciona un an√°lisis ling√º√≠stico del idioma espa√±ol. Este modelo fue elegido por su mayor tama√±o y precisi√≥n en comparaci√≥n con otras versiones m√°s livianas, en el siguiente enlace se pueden encontrar las m√©tricas oficiales de la libreria [spacy](https://spacy.io/models/es).

De igual manera, para los otros campos N√∫mero de identificaci√≥n, correos electr√≥nicos, n√∫mero de telefono y direcciones, se utilizo un m√©todo m√°s sencillo como lo son el reconocimiento de expresiones regulares, las cuales permiten identificar patrones espec√≠ficos en los textos de manera eficiente y automatizada.

Para este modulo se consulta el campo text de la base de datos, el cual se obtiene en el modulo de extracci√≥n de texto, se realiza el procesamiento para obtener las diferentes categorias explicadas anteriormente y se guardan los resultados en el campo pii_entities.

### Identificaci√≥n de documentos duplicados o similares

Para este m√≥dulo se utiliz√≥ el m√©todo sugerido en la gu√≠a, checksum MD5, el cual permite identificar duplicados al generar un valor hash √∫nico para cada archivo o registro. De esta forma, al comparar los valores hash, es posible detectar f√°cilmente entradas id√©nticas y evitar el procesamiento redundante de datos. 

Sin embargo, el problema que presenta chacksum MD5 es que solo permite identificar los archivos que son exactamente iguales, no identifica archivos similares o con peque√±as diferencias, por lo cual tambi√©n se implement√≥ la libreri ssdeep la cual utiliza hashing difuso (fuzzy hashing) para detectar similitudes entre archivos, permitiendo as√≠ identificar duplicados parciales o versiones ligeramente modificadas de un mismo archivo. 

Para este m√≥dulo se requieren directamente los documentos en formato PDF, por lo cual se extraen todos los archivos almacenados en el bucket de S3 y se realizan las comparaciones correspondientes. Finalmente, se almacenan en la base de datos, en los campos extract_duplicates y similar_files, las listas de archivos duplicados y similares respectivamente.

### Segmentaci√≥n y extracci√≥n de secciones de inter√©s.(Opcional)

Para este m√≥dulo se emplearon expresiones regulares dise√±adas espec√≠ficamente para identificar y extraer secciones relevantes dentro de los documentos, tales como referencias a art√≠culos, leyes, decretos, resoluciones y normativas vigentes. Estas expresiones permiten capturar patrones complejos de texto relacionados con disposiciones legales y regulatorias.

No obstante, este m√≥dulo puede mejorarse significativamente aplicando t√©cnicas avanzadas de procesamiento de lenguaje natural (PLN), como el uso de modelos de reconocimiento de entidades nombradas (NER), an√°lisis sem√°ntico y aprendizaje autom√°tico, para lograr una extracci√≥n m√°s precisa y contextualizada, especialmente en casos donde las expresiones regulares tradicionales puedan presentar limitaciones ante variaciones en el lenguaje o formatos menos estructurados.

El funcionamiento es similar al m√≥dulo de detecci√≥n de datos PII. Es decir, se consulta el campo text de la base de datos, el cual se obtiene en el m√≥dulo de extracci√≥n de texto. A partir de este texto, se realiza el procesamiento para identificar y extraer las secciones de inter√©s. Finalmente, se almacenan en el campo normative_section una lista con las secciones relevantes de requisitos normativos encontradas en el texto, o se indica que no aplica en caso de no hallar ninguna secci√≥n pertinente.

## Pautas de uso

La aplicaci√≥n se encuentra desplegada en el siguiente enlace [api](http://meli-ml-challenge-313187819.us-east-2.elb.amazonaws.com/docs#/) donde se puede encontrar la documentaci√≥n interactiva de los endpoints disponibles.Antes de interactuar con cualquiera de ellos, es necesario realizar un proceso de autenticaci√≥n mediante JWT (JSON Web Token), el cual se ha implementado con el fin de proteger los recursos de la API y garantizar que solo usuarios autorizados puedan acceder a las funcionalidades expuestas.Se puede acceder a la api con las siguientes credenciales:

Usuario:user <br>
Contrase√±a:user

#### Autenticaci√≥n 
 
Para realizar la autenticaci√≥n se debe ejecutar el endpoint /api/v1/token, el cual solicita como parametros el usuario y contrase√±a anteriormente mencionados, a continuaci√≥n se muestra un ejemplo de lo que deber√≠a salir en pantalla cuando la autenticaci√≥n es exitosa

![autenticacion](images/autenticacion.png)

El resultado de este endpoint es un token de acceso (JWT), el cual debe ser copiado para poder realizar el proceso de autorizaci√≥n.

#### Autorizaci√≥n

En la parte superior izquierda de la interfaz se encuentra un bot√≥n con la palabra "Authorize". Al hacer clic en este bot√≥n, se despliega una ventana emergente que solicita como par√°metro el token de acceso obtenido en el paso anterior. De la siguiente manera:

![autorizacion](images/autorizacion.png)

Una vez autorizado se podran ejecutar cada uno de los endpoints que se desarrollaron para la presente prueba

#### Extractor de texto

El primer endpoint a probar es el encargado de extraer el texto de diferentes archivos PDF. Este endpoint recibe como par√°metro de entrada una lista de documentos en formato PDF. 

Para efectos de prueba, se enviar√°n tres archivos PDF: certificado1.pdf, certificado12.pdf y resolucion1.pdf. Cabe resaltar que los archivos certificado1.pdf y certificado12.pdf son pr√°cticamente iguales, salvo por una peque√±a modificaci√≥n en el texto realizada intencionalmente. A continuaci√≥n, se muestra un ejemplo de c√≥mo debe estructurarse la petici√≥n:

![extract-text1](images/extract-text1.png)

Si todo sale bien se deberia obtener lo siguiente:

![extract-text2](images/extract-text2.png)

De esta manera, luego de ejecutar este endpoint, los archivos enviados en la solicitud deber√≠an cargarse correctamente en el bucket S3. Al mismo tiempo, en la base de datos se deber√≠an almacenar tres registros, uno por cada archivo procesado. Cada registro deber√≠a contener, el nombre del archivo y el texto extra√≠do del mismo, tal como se muestra a continuaci√≥n

![buckets3](images/buckets3.png)

![bd](images/bd.png)

#### Clasificador de texto

Una vez se haya realizado el proceso de extracci√≥n de texto y los resultados se encuentren almacenados en la base de datos, el endpoint /classify-text se encarga de procesar dicho texto y determinar a qu√© categor√≠a pertenece cada documento. Las categor√≠as posibles son: contrato, certificado, resoluci√≥n y factura.

Para probar este endpoint, se puede enviar como par√°metro una lista opcional de cadenas de texto (tipo list[str]), en la que se especifican los nombres de los archivos que se desean clasificar.

Es importante resaltar que, si la lista se env√≠a vac√≠a ([]), el sistema proceder√° a clasificar todos los registros existentes en la base de datos, como se muestra a continuaci√≥n:

![classify_text1](images/classify_text1.png)

![classify_text2](images/classify_text2.png)

Adem√°s se generar√°n en la base de datos dos nuevos campos, categoria referente a la categor√≠a asignada y los scores, es decir, los puntajes de probabilidad asociados a cada una de las categor√≠as.

![classify_text3](images/classify_text3.png)

#### Detecci√≥n de datos PII

Este modulo al igual que el anterior, recibe como par√°metro una lista opcional de cadenas de texto (tipo list[str]), en la que se especifican los nombres de los archivos en los que se desea realizar la detecci√≥n de datos PII y de igual manera si la lista se encuentra vacia, va a realizar el proceso en todos los regstros de la base de datos:

![detect_pii1](images/detect_pii1.png)

![detect_pii2](images/detect_pii2.png)

Se generaran en la base de datos el campo llamado pii_entities,el cual tendra una lista de las entidas encontradas con su respectivo valor:

![detect_pii3](images/detect_pii3.png)

#### Identificaci√≥n de documentos duplicados o similares

Este m√≥dulo realiza una comparaci√≥n entre todos los documentos PDF almacenados en el bucket S3 con el objetivo de identificar similitudes entre ellos. Adem√°s, recibe como par√°metro el valor de similarity_threshold, que representa el umbral a partir del cual se considera que dos documentos son similares.

Para efectos de la prueba, se utilizar√° un valor de 90 como par√°metro, lo cual indica que si dos documentos presentan una similitud del 90% o m√°s, ser√°n considerados como documentos similares.

Por otro lado, los documentos ser√°n identificados como duplicados √∫nicamente si alcanzan un 100% de similitud, es decir, si son exactamente iguales en contenido.

![duplicates1](images/duplicates1.png)

![duplicates2](images/duplicates2.png)

Como se puede observar en la imagen anterior, respecto a los tres archivos almacenados en el bucket S3, no se encontraron documentos duplicados, ya que ninguno alcanz√≥ el 100% de similitud. Sin embargo, los documentos certificado1.pdf y certificado12.pdf presentan un 91% de similitud, por lo que fueron correctamente identificados como documentos similares seg√∫n el umbral definido en la prueba.

De este modo, tambi√©n se almacenan los resultados en la base de datos en los campos extract_duplicates y similar_files, las listas de archivos duplicados y similares respectivamente.

![duplicates3](images/duplicates3.png)

#### Segmentaci√≥n y extracci√≥n de secciones de inter√©s

Al igual que los m√≥dulos de clasificaci√≥n de texto y extracci√≥n de entidades, este m√≥dulo recibe como par√°metro una lista de nombres de archivos desde los cuales se desea extraer las secciones relacionadas con requisitos normativos. En caso de que la lista se env√≠e vac√≠a ([]), el sistema interpretar√° que se debe realizar la extracci√≥n sobre todos los registros almacenados en la base de datos:

![normative1](images/normative1.png)

![normative2](images/normative2.png)

De igual manera, se registra en la base de datos el campo normative_section, en caso de no encontrar nada relacionado con reuisitos normativos coloca el texto NA

![normative3](images/normative3.png)
