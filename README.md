<h1 align="center">Meli ML Chanllenge üöÄ </h1> 

## Resumen: 
El presente proyecto tiene como fin plantear una posible soluci√≥n para el reto t√©cnico planteado, el cual consiste en dise√±ar y construir un sistema que realice las siguientes tareas a partir de diferentes documentos en formato pdf:<br> 

***1. Clasificaci√≥n autom√°tica por tipo de documento:*** <br>

   Cada archivo debe ser clasificado en una de las siguientes categorias: 
   - Contrato
   - Resoluci√≥n
   - Certificado
   - Factura
2. **Detecci√≥n de datos sensibles (PII)**
     
Identifica y resalta informaci√≥n confidencial como:
- Nombres completos  
- N√∫meros de identificaci√≥n  
- Correos electr√≥nicos  
- Tel√©fonos  
- Direcciones
  
3. **Identificaci√≥n de documentos duplicados o similares**
  
Detecta documentos duplicados por medio de un checksum MD5.

4. **Segmentaci√≥n y extracci√≥n de secciones de inter√©s (Opcional)**

Extrae de cada documento la secci√≥n espec√≠fica que contiene requisitos normativos,  
cuando exista, o marca el documento como ‚ÄúNo Aplica‚Äù.

## Arquitectura y dise√±o de la soluci√≥n

El siguiente diagrama resume c√≥mo est√° construida la presente soluci√≥n:

![Arquitectura](images/Arquitectura.png)

### Desarrollo y construcci√≥n de la aplicaci√≥n 

La aplicaci√≥n fue desarrollada en FastAPI y luego construida como una imagen de docker la cual se almacena en Amazon ECR para su posterior despliegue.

  
### Despliegue de la aplicaci√≥n 

En cuanto al despliegue se utiliz√≥ AWS ECS con Fargate como servicio de orquestaci√≥n para ejecutar la imagen en un entorno serverless, asimismo se utiliz√≥ un Load Balancer que permite el acceso externo a la API.

### Servicios externos conectados 

Se utiliza MongoDB como base de datos para almacenar el texto extra√≠do de los documentos, as√≠ como otros procesamientos realizados (clasificaci√≥n de documentos, extracci√≥n de PII, informaci√≥n de duplicados, extracci√≥n de secciones de inter√©s).

Por otro lado, tambi√©n se cuenta con un bucket de S3 para almacenar los documentos utilizados tanto en el entrenamiento del modelo, que se describe m√°s adelante, como en las diferentes pruebas de la aplicaci√≥n.

## Implementaci√≥n detallada de la soluci√≥n

Para el desarrollo de la aplicaci√≥n, se procur√≥ seguir principios de arquitectura limpia, organizando el proyecto en m√≥dulos con responsabilidades definidas. A continuaci√≥n se explicara cada uno de los modulos:

### Extracci√≥n de texto 
Para abordar el problema planteado, el primer paso realizado fue lograr la extracci√≥n de texto de archivos pdf, para esto se utiliz√≥ amazon textract debido a que ofrece una extracci√≥n robusta basada en machine learning,que no solo reconoce texto impreso, sino tambi√©n estructuras complejas como tablas y formularios.

De igual manera para enfrentar el problema de el procesamiento asincronico se utilizo start_document_text_detection, esta opci√≥n permite iniciar m√∫ltiples procesos de extracci√≥n en paralelo sin necesidad de esperar a que cada uno finalice antes de continuar con el siguiente.

Para gestionar este procesamiento concurrente, se implement√≥ una estrategia que permite lanzar varios trabajos de an√°lisis al mismo tiempo, hacer seguimiento de cada uno de ellos de forma no bloqueante, y finalmente recolectar los resultados una vez completados. Una vez obtenidos los textos de los archivos pdf, se almacenan en la base de datos, lo que permite su posterior uso en tareas como clasificaci√≥n autom√°tica, detecci√≥n de informaci√≥n sensible o an√°lisis de secciones normativas.

### Clasificaci√≥n en categorias

#### Pre procesamiento y entrenamiento del modelo

Con el fin de implementar el sistema clasificador de textos, fueron necesarios unos pasos previos al desarrollo del modulo. En primer lugar se recolectaron diferentes archivos pdf con el objetivo de armar un dataset, de esta manera los documentos referentes a contratos, resoluciones y certificados, se obtuvieron del repositorio de datos abiertos de Secop II [Datos abiertos Secop II](https://www.datos.gov.co/Gastos-Gubernamentales/SECOP-II-Contratos-Electr-nicos/jbjy-vk9h/about_data), de igual manera para las facturas, se utilizaron plantillas de ejemplo.

En total se obtuvieron la siguiente cantidad de documentos, la cantidad de paginas variaba por documento, algunos tenian hasta 15 p√°ginas:

Certificados: 49 <br>
Contratos:43 <br>
Resoluciones: 56 <br>
Facturas: 37 <br>

Una vez recolectados los documentos de entrenamiento, se procedi√≥ a construir el dataset en formato tabular, con el fin de utilizarlo como entrada para los modelos,Este proceso se llev√≥ a cabo mediante un notebook de Jupyter, ubicado en la ruta notebooks/create_tabular_dataset.py, la salida fue un archivo CSV que contiene los campos: filename, label (etiqueta correspondiente a la clase del documento) y text (texto extra√≠do del PDF utilizando Amazon Textract) y se encuentra en la ruta notebooks/data/dataset.csv

Una vez obtenido el dataset tabular, se procedi√≥ a realizar diferentes pruebas utilizando modelos de aprendizaje autom√°tico tradicional. Para ello, se implement√≥ una etapa de vectorizaci√≥n del texto puesto que  Los modelos no entienden palabras directamente, por lo cual la vectorizaci√≥n convierte el texto en representaciones num√©ricas que los algoritmos de machine learning pueden procesar. Se inici√≥ con un modelo de regresi√≥n log√≠stica debido a su simplicidad y buen desempe√±o en problemas de clasificaci√≥n de texto. Los datos se dividieron en un 80 % para entrenamiento y un 20 % para validaci√≥n. A continuaci√≥n, se presentan los resultados obtenidos:

<p align="center">
  <img src="images/mc_regresion1.png" alt="matrizrl1" width="500"/>
</p>

<p align="center">
  <img src="images/metricas_regresion1.png" alt="metricas1" width="500"/>
</p>


Aunque los resultados fueron prometedores, se decidi√≥ mejorar el preprocesamiento del texto incorporando una peque√±a etapa de limpieza y tokenizaci√≥n del texto utilizando la libreria de procesamiento de lenguaje natural nltk para optimizar la calidad de las caracter√≠sticas extra√≠das. Adem√°s se decidi√≥ evaluar varios modelos de clasificaci√≥n como adicional a la regresi√≥n logistica como Lineal SVM, Random Forest, Multinomial Naive Bayes, con el fin de comparar y elegir el que brindara mejores m√©tricas. A continuaci√≥n se muestran los resultados de cada uno de los modelos con la limpieza de texto realizada

Regresi√≥n Logistica

<p align="center">
  <img src="images/mc_regresion2.png" alt="matrizrl2" width="500"/>
</p>

<p align="center">
  <img src="images/metricas_regresion2.png" alt="metricas2" width="500"/>
</p>


SVM

<p align="center">
  <img src="images/mc_svm.png" alt="matrizsvm" width="500"/>
</p>

<p align="center">
  <img src="images/metricas_SVM.png" alt="metricas3" width="500"/>
</p>


Random Forest

<p align="center">
  <img src="images/mc_randomforest.png" alt="matrizrandom" width="500"/>
</p>

<p align="center">
  <img src="images/metricas_randomforest.png" alt="metricas4" width="500"/>
</p>

NB

<p align="center">
  <img src="images/mc_NB.png" alt="matriznb" width="500"/>
</p>

<p align="center">
  <img src="images/metricas_NB.png" alt="metricas1" width="500"/>
</p>

Como se puede observar, en t√©rminos generales y para todas las m√©tricas (Accuracy, Precision, Recall y F1-score), el modelo Random Forest alcanz√≥ un desempe√±o cercano al 97%. Por ello, fue seleccionado para su implementaci√≥n en el m√≥dulo de clasificaci√≥n de categor√≠as.

#### Implementaci√≥n modulo clasificador

Una vez obtenido el modelo clasificador, se desarroll√≥ el m√≥dulo encargado de determinar la categor√≠a que mejor se ajusta al documento. Para ello, se consulta en la base de datos el texto extra√≠do de los PDFs y se realiza la clasificaci√≥n correspondiente. Finalmente, el m√≥dulo almacena dos campos: la categor√≠a asignada y los scores, es decir, los puntajes de probabilidad asociados a cada una de las categor√≠as.

### Detecci√≥n de datos sensibles (PII)

Para este m√≥dulo se utiliz√≥ la biblioteca spaCy para el reconocimiento de entidades nombradas (NER), espec√≠ficamente para la categor√≠a de nombres propios. De igual manera, se implement√≥ el modelo es_core_news_lg de la misma biblioteca, el cual proporciona un an√°lisis ling√º√≠stico del idioma espa√±ol. Este modelo fue elegido por su mayor tama√±o y precisi√≥n en comparaci√≥n con otras versiones m√°s livianas, en el siguiente enlace se pueden encontrar las m√©tricas oficiales de la libreria [spacy](https://spacy.io/models/es).

De igual manera, para los otros campos N√∫mero de identificaci√≥n, correos electr√≥nicos, n√∫mero de telefono y direcciones, se utilizo un m√©todo m√°s sencillo como lo son el reconocimiento de expresiones regulares, las cuales permiten identificar patrones espec√≠ficos en los textos de manera eficiente y automatizada.

Para este modulo se consulta el campo text de la base de datos, el cual se obtiene en el modulo de extracci√≥n de texto, se realiza el procesamiento para obtener las diferentes categorias explicadas anteriormente y se guardan los resultados en el campo pii_entities.

### Identificaci√≥n de documentos duplicados o similares

Para este m√≥dulo se utiliz√≥ el m√©todo sugerido en la gu√≠a, checksum MD5, el cual permite identificar duplicados al generar un valor hash √∫nico para cada archivo o registro. De esta forma, al comparar los valores hash, es posible detectar f√°cilmente entradas id√©nticas y evitar el procesamiento redundante de datos. 

Sin embargo, el problema que presenta chacksum MD5 es que solo permite identificar los archivos que son exactamente iguales, no identifica archivos similares o con peque√±as diferencias, por lo cual tambi√©n se implement√≥ la libreri ssdeep la cual utiliza hashing difuso (fuzzy hashing) para detectar similitudes entre archivos, permitiendo as√≠ identificar duplicados parciales o versiones ligeramente modificadas de un mismo archivo. 

Para este m√≥dulo se requieren directamente los documentos en formato PDF, por lo cual se extraen todos los archivos almacenados en el bucket de S3 y se realizan las comparaciones correspondientes. Finalmente, se almacenan en la base de datos, en los campos extract_duplicates y similar_files, las listas de archivos duplicados y similares respectivamente.

### Segmentaci√≥n y extracci√≥n de secciones de inter√©s.(Opcional)

Para este m√≥dulo se emplearon expresiones regulares dise√±adas espec√≠ficamente para identificar y extraer secciones relevantes dentro de los documentos, tales como referencias a art√≠culos, leyes, decretos, resoluciones y normativas vigentes. Estas expresiones permiten capturar patrones complejos de texto relacionados con disposiciones legales y regulatorias.

No obstante, este m√≥dulo puede mejorarse significativamente aplicando t√©cnicas avanzadas de procesamiento de lenguaje natural (PLN), como el uso de modelos de reconocimiento de entidades nombradas (NER), an√°lisis sem√°ntico y aprendizaje autom√°tico, para lograr una extracci√≥n m√°s precisa y contextualizada, especialmente en casos donde las expresiones regulares tradicionales puedan presentar limitaciones ante variaciones en el lenguaje o formatos menos estructurados.

El funcionamiento es similar al m√≥dulo de detecci√≥n de datos PII. Es decir, se consulta el campo text de la base de datos, el cual se obtiene en el m√≥dulo de extracci√≥n de texto. A partir de este texto, se realiza el procesamiento para identificar y extraer las secciones de inter√©s. Finalmente, se almacenan en el campo normative_section una lista con las secciones relevantes de requisitos normativos encontradas en el texto, o se indica que no aplica en caso de no hallar ninguna secci√≥n pertinente.



   
